{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d592d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import nltk\n",
    "from datasets import load_metric\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import evaluate\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_excel('Summary_training.xlsx')\n",
    "df.columns = ['notes', 'summary']\n",
    "\n",
    "# Convert the DataFrame to a Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7324ad50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX A1000 6GB Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "#Check GPU Availability\n",
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45d311a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406,290,432 total parameters.\n",
      "406,290,432 training parameters.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'facebook/bart-large-cnn'\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfbbe555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698cc23128c6426ea1252ec1da92bb3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adpur\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:3935: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8407a83f55c467d810212192790ea7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_data(tokenizer, examples):\n",
    "    inputs = [doc for doc in examples['notes']]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=1024,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    targets = [summary for summary in examples['summary']]\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=1024,\n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        )\n",
    " \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Apply the function to the whole dataset\n",
    "train_dataset = train_dataset.map(lambda x: preprocess_data(tokenizer,x), batched=True)\n",
    "val_dataset = val_dataset.map(lambda x: preprocess_data(tokenizer,x), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "330164f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f61407c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred.predictions[0], eval_pred.label_ids\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(\n",
    "        predictions=decoded_preds, \n",
    "        references=decoded_labels, \n",
    "        use_stemmer=True, \n",
    "        rouge_types=[\n",
    "            'rouge1', \n",
    "            'rouge2', \n",
    "            'rougeL'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60ee3a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Original Trainer may have a memory leak. \n",
    "    This is a workaround to avoid storing too many tensors that are not needed.\n",
    "    \"\"\"\n",
    "    pred_ids = torch.argmax(logits[0], dim=-1)\n",
    "    return pred_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3128cadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,    \n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=5,\n",
    "    save_strategy='steps',\n",
    "    save_total_limit=2,\n",
    "    report_to='tensorboard',\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True if torch.cuda.is_available() else False,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b123451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "C:\\Users\\adpur\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\bart\\modeling_bart.py:590: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='145' max='145' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [145/145 08:07, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>11.163314</td>\n",
       "      <td>0.229900</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>0.192500</td>\n",
       "      <td>1024.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>11.041400</td>\n",
       "      <td>10.243776</td>\n",
       "      <td>0.194300</td>\n",
       "      <td>0.109100</td>\n",
       "      <td>0.175900</td>\n",
       "      <td>1024.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>11.041400</td>\n",
       "      <td>9.763458</td>\n",
       "      <td>0.414700</td>\n",
       "      <td>0.227900</td>\n",
       "      <td>0.383100</td>\n",
       "      <td>1024.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>9.458500</td>\n",
       "      <td>8.691725</td>\n",
       "      <td>0.472100</td>\n",
       "      <td>0.271700</td>\n",
       "      <td>0.435800</td>\n",
       "      <td>1024.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>9.458500</td>\n",
       "      <td>7.145017</td>\n",
       "      <td>0.319800</td>\n",
       "      <td>0.192300</td>\n",
       "      <td>0.297300</td>\n",
       "      <td>1024.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>7.007300</td>\n",
       "      <td>5.682806</td>\n",
       "      <td>0.206000</td>\n",
       "      <td>0.124400</td>\n",
       "      <td>0.181400</td>\n",
       "      <td>845.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>7.007300</td>\n",
       "      <td>2.999268</td>\n",
       "      <td>0.484300</td>\n",
       "      <td>0.258000</td>\n",
       "      <td>0.438500</td>\n",
       "      <td>289.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.476200</td>\n",
       "      <td>1.685207</td>\n",
       "      <td>0.601500</td>\n",
       "      <td>0.334000</td>\n",
       "      <td>0.550500</td>\n",
       "      <td>66.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>3.476200</td>\n",
       "      <td>1.041592</td>\n",
       "      <td>0.637800</td>\n",
       "      <td>0.333900</td>\n",
       "      <td>0.569000</td>\n",
       "      <td>59.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.459900</td>\n",
       "      <td>0.706010</td>\n",
       "      <td>0.641500</td>\n",
       "      <td>0.335600</td>\n",
       "      <td>0.568800</td>\n",
       "      <td>57.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.459900</td>\n",
       "      <td>0.508076</td>\n",
       "      <td>0.647300</td>\n",
       "      <td>0.349600</td>\n",
       "      <td>0.587200</td>\n",
       "      <td>56.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.690800</td>\n",
       "      <td>0.387101</td>\n",
       "      <td>0.674400</td>\n",
       "      <td>0.391000</td>\n",
       "      <td>0.627200</td>\n",
       "      <td>55.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.690800</td>\n",
       "      <td>0.310422</td>\n",
       "      <td>0.691300</td>\n",
       "      <td>0.414400</td>\n",
       "      <td>0.646100</td>\n",
       "      <td>55.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.428900</td>\n",
       "      <td>0.258782</td>\n",
       "      <td>0.701600</td>\n",
       "      <td>0.418000</td>\n",
       "      <td>0.642400</td>\n",
       "      <td>55.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.428900</td>\n",
       "      <td>0.215451</td>\n",
       "      <td>0.698500</td>\n",
       "      <td>0.415600</td>\n",
       "      <td>0.637300</td>\n",
       "      <td>55.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.282900</td>\n",
       "      <td>0.184646</td>\n",
       "      <td>0.709000</td>\n",
       "      <td>0.442900</td>\n",
       "      <td>0.652300</td>\n",
       "      <td>55.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.282900</td>\n",
       "      <td>0.161690</td>\n",
       "      <td>0.710900</td>\n",
       "      <td>0.445300</td>\n",
       "      <td>0.654500</td>\n",
       "      <td>55.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.209300</td>\n",
       "      <td>0.143249</td>\n",
       "      <td>0.713900</td>\n",
       "      <td>0.474100</td>\n",
       "      <td>0.661500</td>\n",
       "      <td>55.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.209300</td>\n",
       "      <td>0.128183</td>\n",
       "      <td>0.720900</td>\n",
       "      <td>0.513500</td>\n",
       "      <td>0.677700</td>\n",
       "      <td>55.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.118424</td>\n",
       "      <td>0.728300</td>\n",
       "      <td>0.526300</td>\n",
       "      <td>0.689000</td>\n",
       "      <td>55.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.112538</td>\n",
       "      <td>0.726800</td>\n",
       "      <td>0.522500</td>\n",
       "      <td>0.687300</td>\n",
       "      <td>55.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.131800</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>0.741300</td>\n",
       "      <td>0.549100</td>\n",
       "      <td>0.705400</td>\n",
       "      <td>55.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.131800</td>\n",
       "      <td>0.099324</td>\n",
       "      <td>0.750300</td>\n",
       "      <td>0.572200</td>\n",
       "      <td>0.726500</td>\n",
       "      <td>55.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.108100</td>\n",
       "      <td>0.094517</td>\n",
       "      <td>0.748900</td>\n",
       "      <td>0.567500</td>\n",
       "      <td>0.723900</td>\n",
       "      <td>55.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.108100</td>\n",
       "      <td>0.092124</td>\n",
       "      <td>0.752700</td>\n",
       "      <td>0.575100</td>\n",
       "      <td>0.726500</td>\n",
       "      <td>55.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.101500</td>\n",
       "      <td>0.089022</td>\n",
       "      <td>0.764700</td>\n",
       "      <td>0.577000</td>\n",
       "      <td>0.730100</td>\n",
       "      <td>55.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.101500</td>\n",
       "      <td>0.087203</td>\n",
       "      <td>0.751300</td>\n",
       "      <td>0.574100</td>\n",
       "      <td>0.726500</td>\n",
       "      <td>55.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>0.086768</td>\n",
       "      <td>0.751300</td>\n",
       "      <td>0.574100</td>\n",
       "      <td>0.726500</td>\n",
       "      <td>55.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>0.086234</td>\n",
       "      <td>0.752700</td>\n",
       "      <td>0.578900</td>\n",
       "      <td>0.726500</td>\n",
       "      <td>55.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_bart\\\\tokenizer_config.json',\n",
       " './fine_tuned_bart\\\\special_tokens_map.json',\n",
       " './fine_tuned_bart\\\\vocab.json',\n",
       " './fine_tuned_bart\\\\merges.txt',\n",
       " './fine_tuned_bart\\\\added_tokens.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained('./fine_tuned_bart')\n",
    "tokenizer.save_pretrained('./fine_tuned_bart')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb0a38ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART ROUGE scores: {'eval_loss': 0.08623351901769638, 'eval_rouge1': 0.7527, 'eval_rouge2': 0.5789, 'eval_rougeL': 0.7265, 'eval_gen_len': 55.5, 'eval_runtime': 4.3203, 'eval_samples_per_second': 0.926, 'eval_steps_per_second': 0.926, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the models\n",
    "eval_results_bart = trainer.evaluate(eval_dataset=val_dataset)\n",
    "print(\"BART ROUGE scores:\", eval_results_bart)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6e2916",
   "metadata": {},
   "source": [
    "### ok lets test now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "502cbf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text:\n",
      "Update On:1st Jan 2023, Per update from John Smith (AP lead ABC Corp), payment is expected in monthly installments of 100k USD for all current open invoices starting Feb.Update On:5th Feb 2023, First installment of payment is received and applied against oldest invoices. Further payment is awaited.Update On:3rd Mar 2023, Connected with John to get update on payment. The next installment is expected in a week as per update from John.Update On:13th Mar 2023, Next installment of 100k received and applied.Update On:28th Mar 2023, Received email from John regarding inability to make April payment. Further updates on next installment is awaited.Update On:15th Apr 2023, As of 14th April, 20 open invoices are awaiting payment total 500k. Made contact with Jim (EP) and Jim will be following up directly with client.Update On:28th Apr 2023, Lump sum settlment has been made and applied to all open AR.\n",
      "\n",
      "Base BART Summary:\n",
      "Payment is expected in monthly installments of 100k USD for all current open invoices starting Feb. As of 14th April, 20 open Invoices are awaiting payment total 500k.\n",
      "\n",
      "Fine-Tuned BART Summary:\n",
      "As per update from AP lead, payment was expected in monthly installments of 100k USD each starting Feb 23. First installment of payment was received on 5th Feb and applied against oldest invoices. Further updates on next installment is awaited. As of 14th April, 20 open AR were awaiting payment total 500k\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load base BART model and tokenizer using pipeline\n",
    "base_bart = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Load fine-tuned BART model and tokenizer using pipeline\n",
    "fine_tuned_bart = pipeline(\"summarization\", model=\"./fine_tuned_bart\")\n",
    "\n",
    "# Load tokenizers for preprocessing\n",
    "base_bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "fine_bart_tokenizer = BartTokenizer.from_pretrained(\"./fine_tuned_bart\")\n",
    "\n",
    "# Example input text (use one of the notes from your dataset)\n",
    "input_text = df['notes'][0]\n",
    "\n",
    "\n",
    "base_bart_inputs = base_bart_tokenizer(input_text, return_tensors='pt', max_length=1024, truncation=True, padding='max_length')\n",
    "fine_bart_inputs = fine_bart_tokenizer(input_text, return_tensors='pt', max_length=1024, truncation=True, padding='max_length')\n",
    "\n",
    "# Generate summaries using base and fine-tuned BART models\n",
    "base_bart_summary = base_bart_tokenizer.decode(base_bart.model.generate(base_bart_inputs['input_ids'], max_length=250, min_length=40, length_penalty=2, num_beams=6)[0], skip_special_tokens=True)\n",
    "fine_bart_summary = fine_bart_tokenizer.decode(fine_tuned_bart.model.generate(fine_bart_inputs['input_ids'], max_length=250, min_length=40, length_penalty=2, num_beams=6)[0], skip_special_tokens=True)\n",
    "\n",
    "#base_bart_summary = base_bart(input_text, max_length=250, min_length=40, length_penalty=2, num_beams=6)[0]['summary_text']\n",
    "#fine_bart_summary = fine_tuned_bart(input_text, max_length=250, min_length=40, length_penalty=2, num_beams=6)[0]['summary_text']\n",
    "\n",
    "print(\"Input Text:\")\n",
    "print(input_text)\n",
    "print(\"\\nBase BART Summary:\")\n",
    "print(base_bart_summary)\n",
    "print(\"\\nFine-Tuned BART Summary:\")\n",
    "print(fine_bart_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f9d30f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text:\n",
      "Update On:20th Jan 2024, Sent initial follow-up for payments for Invoices AC123 and AC345.Update On:23rd Jan 2024, Got response from XYZ Billing team asking for clarification on $500 sales tax amount on AC123. Forwarded correspondance to Billing Team to get breakdown.Update On:25th Jan 2024, Mike from the engagement billing team has provided breakdown and the same has been forwarded to XYZ Billing team.Update On:1st Feb 2024, XYZ Billing team has sought a call to further discuss the sales tax discrepancy and get confirmation on similar amounts in other open AR. Meeting has been setup for later this week.Update On:8th Feb 2024, Follow-up meeting will be setup for 3rd week of Feb once the provided data has been analyzed by XYZ team.Update On:21st Feb 2024, XYZ would like to dispute the sales tax and review all open services with our firm. Per discussion with Will (EP), no further correspondance will be conducted with XYZ by the collections team.Update On:28th Feb 2024, All open invoices will be cancelled and written off. No further payment is expected.\n",
      "\n",
      "Base BART Summary:\n",
      "Update On:20th Jan 2024, Sent initial follow-up for payments for Invoices AC123 and AC345. Got response from XYZ Billing team asking for clarification on $500 sales tax amount on AC123. Forwarded correspondance to Billing Team to get breakdown.\n",
      "\n",
      "Fine-Tuned BART Summary:\n",
      "An initial follow-up was done with XYZ Billing team regarding $500 sales tax on Invoice AC123. After a series of meetings, the provided breakdown was provided to the team from engagement billing team and they sought a call to further discuss the sales tax discrepancy and get confirmation on similar amounts in other open AR. After discussion with Will (EP), no further correspondance was conducted withXYZ by collections team and all open invoices were cancelled and written off.\n"
     ]
    }
   ],
   "source": [
    "# Example input text (use one of the notes from your dataset)\n",
    "input_text = df['notes'][1]\n",
    "\n",
    "\n",
    "base_bart_inputs = base_bart_tokenizer(input_text, return_tensors='pt', max_length=1024, truncation=True, padding='max_length')\n",
    "fine_bart_inputs = fine_bart_tokenizer(input_text, return_tensors='pt', max_length=1024, truncation=True, padding='max_length')\n",
    "\n",
    "# Generate summaries using base and fine-tuned BART models\n",
    "base_bart_summary = base_bart_tokenizer.decode(base_bart.model.generate(base_bart_inputs['input_ids'], max_length=250, min_length=40, length_penalty=2, num_beams=6)[0], skip_special_tokens=True)\n",
    "fine_bart_summary = fine_bart_tokenizer.decode(fine_tuned_bart.model.generate(fine_bart_inputs['input_ids'], max_length=250, min_length=40, length_penalty=2, num_beams=6)[0], skip_special_tokens=True)\n",
    "\n",
    "#base_bart_summary = base_bart(input_text, max_length=250, min_length=40, length_penalty=2, num_beams=6)[0]['summary_text']\n",
    "#fine_bart_summary = fine_tuned_bart(input_text, max_length=250, min_length=40, length_penalty=2, num_beams=6)[0]['summary_text']\n",
    "\n",
    "print(\"Input Text:\")\n",
    "print(input_text)\n",
    "print(\"\\nBase BART Summary:\")\n",
    "print(base_bart_summary)\n",
    "print(\"\\nFine-Tuned BART Summary:\")\n",
    "print(fine_bart_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a853dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
