{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8a39a5b",
   "metadata": {},
   "source": [
    "# Codesphere - Hackathon working notebook\n",
    "\n",
    "This notebook will serve as a working prototype for our hackathon problem statement. \n",
    "\n",
    "**Problem Statement:** Notes Summarizer for an Invoice collections application. \n",
    "\n",
    "**Description:** Our aim is to create an AI generated summary of all the notes added against a given client. The summary will save time for the collections agent as he/she will not be required to peruse all the notes available in the system for a given client to get the gist of the client standing and past history. The summary should be able to identify the key points from past notes, summarize them in a cohesive and readable manner and display them in not more than 2 paragraphs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecf14c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\vsidd\\anaconda3\\lib\\site-packages (4.38.2)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\vsidd\\anaconda3\\lib\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: requests in c:\\users\\vsidd\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\vsidd\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\vsidd\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vsidd\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\vsidd\\anaconda3\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vsidd\\anaconda3\\lib\\site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\vsidd\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\vsidd\\anaconda3\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\vsidd\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vsidd\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\vsidd\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vsidd\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vsidd\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\vsidd\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vsidd\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\vsidd\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2251236b",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "179f72f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee381dcf",
   "metadata": {},
   "source": [
    "We will initialize the tokenizer with a base model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e73f32d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118fe303",
   "metadata": {},
   "source": [
    "For phase 1, we will main note related data in an input file and load it. The file has the below columns\n",
    "\n",
    "* Client name - This will be our primary reference.\n",
    "* Note Date - When this particular note was added. \n",
    "* Note Comment - The actual comment data that needs to be summarized. \n",
    "\n",
    "Note that the file can have multiple notes for the same client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54eb0257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel file\n",
    "df = pd.read_excel('Dataset.xlsx', engine='openpyxl')\n",
    "df.columns = ['Client Name', 'Note Date', 'Note Comment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1c019d",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We shall not enter in-depth into data preprocessing for phase 1, but what we will do is to add a date reference to the note input by concatenating the note date with the comment. \n",
    "\n",
    "In order to make it more human readable, we will add some formatting. Hence, 01-Jan-2024 will read as 1st Jan and 15-Jan-2024 will read as 15th Jan. As there is no in-built function to add the ordinal indicator we want using [grammarly](https://www.grammarly.com/blog/how-to-write-ordinal-numbers-correctly/) as a reference :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "604ee3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ordinal_indicator(date):\n",
    "    day = date.day\n",
    "    if 4 <= day <= 20 or 24 <= day <= 30:\n",
    "        suffix = \"th\"\n",
    "    else:\n",
    "        suffix = [\"st\", \"nd\", \"rd\"][day % 10 - 1]\n",
    "    return f\"On {day}{suffix} {date.strftime('%b %Y')}, \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f35ac78",
   "metadata": {},
   "source": [
    "Now to concatenate the notes itself... We will create a data dictionary to hold the output for every client. Finally we will pass the output to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c361968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client_notes = {}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    client = row['Client Name']\n",
    "    \n",
    "    # If this is the first note for this client, create a new list for them\n",
    "    if client not in client_notes:\n",
    "        client_notes[client] = ''\n",
    "    \n",
    "    #Siddesh - Added this check as empty note was giving summarizing issues. \n",
    "    if row['Note Comment'] is not None:\n",
    "        # Format the date and prepend it to the note\n",
    "        note_date = add_ordinal_indicator(row['Note Date'])\n",
    "        note_with_date = note_date + row['Note Comment']\n",
    "        client_notes[client] += note_with_date + ' '\n",
    "\n",
    "summary_df = pd.DataFrame(columns=['Client Name', 'Summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcb0604",
   "metadata": {},
   "source": [
    "# Generating the summary\n",
    "\n",
    "We will now ask the model to generate summarized text for each client with a 150 char limit. We prefix the concatenated notes with a prompt \"Summary:\" to let the model know that we are expecting summarized output. This gets saved to a new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4cef626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vsidd\\AppData\\Local\\Temp\\ipykernel_35380\\703304183.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  summary_df = summary_df.append({'Client Name': client, 'Summary': summary}, ignore_index=True)\n",
      "C:\\Users\\vsidd\\AppData\\Local\\Temp\\ipykernel_35380\\703304183.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  summary_df = summary_df.append({'Client Name': client, 'Summary': summary}, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Now you can generate the summary for each client and add it to the DataFrame. \n",
    "for client, notes in client_notes.items():\n",
    "   \n",
    "    inputs = tokenizer.encode(\"summarize: \" + notes, return_tensors='pt', max_length=512, truncation=True)\n",
    "\n",
    "    outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "    # Siddesh: Output is giving padding tags which we can remove for now. \n",
    "    summary = tokenizer.decode(outputs[0]).replace('<pad>', '').replace('</s>', '')\n",
    "\n",
    "    summary_df = summary_df.append({'Client Name': client, 'Summary': summary}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0279cb",
   "metadata": {},
   "source": [
    "# Saving the output to file\n",
    "\n",
    "For phase 1 - We will save the output back to our dataset file under summary tab. Note that we will overwrite any old output as each run is considered fresh. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daaa0a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('Dataset.xlsx', engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "    summary_df.to_excel(writer, sheet_name='Summary', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c09409",
   "metadata": {},
   "source": [
    "# Summary analysis\n",
    "\n",
    "Let us have a peek into the summary generated by the model and see how the output compares to the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ef07095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text:\n",
      "On 1st Jan 2024, Reached out to client AP contact for payment of 10 open invoices totaling 10250 USD. On 15th Jan 2024, Client advised that there is cash crunch impacting fund transfer. Partial payment expected by Jan Month end with further settlements in Feb On 2nd Feb 2024, As discussed, partial payment of 5,000 USD received via wire transfer.  On 14th Feb 2024, Had further follow-up with client on remaining open balance. This includes newly created 5 invoices with a value of 4000 USD leading to open balance of 12250 USD.  On 1st Mar 2024, Client has released another partial payment of 8000 USD. Had discussion with AP contact on settlement plan for current open balance.  On 15th Mar 2024, AP Contact has been changed from John to Matthew effective immediately. Matthew will be the SPOC for all payments going forward.  On 1st Apr 2024, Client has released full payment for all open invoices, effectively closing out the dunning process for current open AR. Expect all payments to be applied by 5th April in the system.  \n",
      "Output summary:\n",
      " on 1st Jan 2024, Reached out to client AP contact for payment of 10 open invoices. on 15th Jan 2024, Client advised that there is cash crunch impacting fund transfer. on 1st Apr 2024, Client has released full payment for all open invoices.\n"
     ]
    }
   ],
   "source": [
    "print(\"Input Text:\")\n",
    "print(client_notes['A'])\n",
    "print(\"Output summary:\")\n",
    "print(summary_df['Summary'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e05dbee",
   "metadata": {},
   "source": [
    "As we can see, the summary in question is extractive in nature - Meaning that the model has actually used the input data passed and picked key points and stitched them together to generate a summary. \n",
    "\n",
    "Now, this is a start, but we eventually want it to generate abstract summary so that it can add new words to form a more cohesive summary. \n",
    "\n",
    "T5 does have this capability - But we will need to fine tune it for abstractive summarization by training it on a relevant dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2bf0e6",
   "metadata": {},
   "source": [
    "# Preparing a dataset to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc9f519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
